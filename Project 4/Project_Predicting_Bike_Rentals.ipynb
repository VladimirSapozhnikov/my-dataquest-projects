{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many U.S. cities have bike sharing stations, where bikes can be rented by the hour or day.\n",
    "#We will be working with data on bike rentals in Washington D.C.\n",
    "\n",
    "#Our objective is to try to predict the total number of bikes people rent in a given hour based \n",
    "#on factors such as temperature, month, whether its a holiday or not etc.\n",
    "\n",
    "#In this project, the emphasis will be on trying different machine learning models and optimising \n",
    "#their parameters, in order to achieve the most accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentals = pd.read_csv(\"hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_rentals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the column descriptions can be found here: http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6972., 3705., 2659., 1660.,  987.,  663.,  369.,  188.,  139.,\n",
       "          37.]),\n",
       " array([  1. ,  98.6, 196.2, 293.8, 391.4, 489. , 586.6, 684.2, 781.8,\n",
       "        879.4, 977. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(bike_rentals[\"cnt\"]) #distribution of cnt (total n. bike rentals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can do some work with our features to enhance the accuracy of our models.\n",
    "#For example the hour column contains values from 1 to 24. We can group this information better by\n",
    "#creating new column labels  such as morning, afternoon, evening and night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(h):\n",
    "    if h >= 6 and h < 12:\n",
    "        return 1\n",
    "    elif h >= 12 and h< 18:\n",
    "        return 2\n",
    "    elif h >= 18 and h <= 24:\n",
    "        return 3\n",
    "    elif h >= 0 and h < 6:\n",
    "        return 4\n",
    "    \n",
    "bike_rentals[\"time_label\"] = bike_rentals[\"hr\"].apply(assign_label)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No we can begin preparing our data to be used with machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "\n",
    "train = bike_rentals.sample(frac = 0.8)\n",
    "test = bike_rentals[~bike_rentals.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Selection Stage.\n",
    "#Here we discuss and try different ml models to find the most appropriate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant       0.278379\n",
       "season        0.178056\n",
       "yr            0.250495\n",
       "mnth          0.120638\n",
       "hr            0.394071\n",
       "holiday      -0.030927\n",
       "weekday       0.026900\n",
       "workingday    0.030284\n",
       "weathersit   -0.142426\n",
       "temp          0.404772\n",
       "atemp         0.400929\n",
       "hum          -0.322911\n",
       "windspeed     0.093234\n",
       "casual        0.694564\n",
       "registered    0.972151\n",
       "cnt           1.000000\n",
       "time_label   -0.378318\n",
       "Name: cnt, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Regression\n",
    "#To see if using linear regression is appropriate for this data, we look at how our columns correlate\n",
    "#with the target column (cnt). \n",
    "\n",
    "#Linear regression is used when columns are linearly correlated with with the target column, however\n",
    "#linear models are prone to underfitting (not describing the actual relationships between columns by \n",
    "#oversimplifying them).\n",
    "\n",
    "bike_rentals.corr()[\"cnt\"] #looking at how each column is correlated with cnt (total n. bike rentals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring the casual and registered columns which will not be used in our analysis (since they already \n",
    "#tell the model how many people use the bikes in a given hour), we have some columns that correlate\n",
    "#quite highly with the cnt column.\n",
    "\n",
    "#This means we could try linear regression.\n",
    "\n",
    "#Creating a list of predictor columns.\n",
    "train_cols = list(bike_rentals)\n",
    "#remove some columns initially.\n",
    "train_cols.remove(\"cnt\")\n",
    "train_cols.remove(\"casual\")\n",
    "train_cols.remove(\"registered\")\n",
    "train_cols.remove(\"dteday\")\n",
    "train_cols.remove(\"atemp\")\n",
    "\n",
    "target = train[\"cnt\"]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instant',\n",
       " 'season',\n",
       " 'yr',\n",
       " 'mnth',\n",
       " 'hr',\n",
       " 'holiday',\n",
       " 'weekday',\n",
       " 'workingday',\n",
       " 'weathersit',\n",
       " 'temp',\n",
       " 'hum',\n",
       " 'windspeed',\n",
       " 'time_label']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train_test(train_col, train, test):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    lr.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_lr = lr.predict(test[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(test[\"cnt\"], prediction_lr)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16955.253895482252"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_train_test(train_cols, train, test) #the error is very high, indicating that the model\n",
    "#is an underfit, which was mentioned as one of the drawbacks of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "#Decision trees tend to be more reliable ml models, as they are able to capture non-linear\n",
    "#relationships between our variables. \n",
    "\n",
    "#They also tend to be quite sensitive, meaning they are greatly affected by noise, and as a result\n",
    "#they are prone to overfitting.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_train_test(train_col, train, test):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    tree = DecisionTreeRegressor()\n",
    "    \n",
    "    tree.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_tree = tree.predict(test[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(test[\"cnt\"], prediction_tree)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334.4024741081703"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_train_test(train_cols, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The decision tree model with default parameters performs much better than the linear regression \n",
    "#model as evident by the much smaller value of mse.\n",
    "\n",
    "#Perhaps it is possible to improve the model by optimising some of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_train_test(train_col, train, test, def_min_samples_leaf = 5):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(min_samples_leaf = def_min_samples_leaf)\n",
    "    \n",
    "    tree.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_tree = tree.predict(test[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(test[\"cnt\"], prediction_tree)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3334.4024741081703,\n",
       " 2: 2824.767612837233,\n",
       " 3: 2703.3289930155993,\n",
       " 4: 2587.8314488514047,\n",
       " 5: 2528.433272036895,\n",
       " 6: 2595.4851416106867,\n",
       " 7: 2600.281147004168,\n",
       " 8: 2579.433352238942,\n",
       " 9: 2634.3991955764864,\n",
       " 10: 2638.4552130041516,\n",
       " 11: 2648.99503621641,\n",
       " 12: 2724.073404802602,\n",
       " 13: 2762.297642753705,\n",
       " 14: 2760.997283870724}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses = {}\n",
    "for i in range(1,15):\n",
    "    mses[i] = tree_train_test(train_cols, train, test, def_min_samples_leaf=i)\n",
    "mses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the model performs the best with the min_samples_leaf parameter set at 5.\n",
    "\n",
    "#Then our model achieves a value of mse roughly equal to 2528 which is a lot better than\n",
    "#17000 mse, achieved by linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now check if there are signs of overfitting in our Decision tree model.\n",
    "#The big indicator is usually when a model is tested on the data it was trained on, it tends to \n",
    "#perform much better on it than on the testing data.\n",
    "\n",
    "#This would indicate that a model is paying too much attention to the random noise in the traning set\n",
    "#as consequently cannot solve the general problem as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_train_test_check(train_col, train, test, def_min_samples_leaf = 5):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(min_samples_leaf = def_min_samples_leaf)\n",
    "    \n",
    "    tree.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_tree = tree.predict(train[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(train[\"cnt\"], prediction_tree)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0,\n",
       " 2: 329.06759931909187,\n",
       " 3: 636.7134623222806,\n",
       " 4: 859.9895217887198,\n",
       " 5: 1073.592044368636,\n",
       " 6: 1225.9137364956684,\n",
       " 7: 1363.1425185334379,\n",
       " 8: 1472.2768031703765,\n",
       " 9: 1568.5974562004199,\n",
       " 10: 1661.2918409947918,\n",
       " 11: 1736.0997058355708,\n",
       " 12: 1816.1084535062641,\n",
       " 13: 1870.202656622492,\n",
       " 14: 1964.9972557349863}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses = {}\n",
    "for i in range(1,15):\n",
    "    mses[i] = tree_train_test_check(train_cols, train, test, def_min_samples_leaf=i)\n",
    "mses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As expected, the model performs much better on the train set, indicating that overfitting is \n",
    "#taking place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor\n",
    "#random forests are collections of decision trees, with slightly altered parameters, which makes them\n",
    "#really accurate and also less prone to overfitting.\n",
    "\n",
    "#One disadvantage of random forest algorithms is that they tend to be computationally expensive.\n",
    "#Constructing a random forest with 2 trees, takes twice as long as the decision tree algorithm,\n",
    "#and random forests usually consist of tens of trees.\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_train_test(train_col, train, test):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    rf.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_rf = rf.predict(test[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(test[\"cnt\"], prediction_rf)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1625.6495998849252"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_train_test(train_cols, train, test) #mse with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets perform some parameter optimisation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_train_test(train_col, train, test, def_min_samples_leaf = 1, def_n_estimators = 200):\n",
    "    np.random.seed(101)\n",
    "    \n",
    "    rf = RandomForestRegressor(min_samples_leaf = def_min_samples_leaf, \n",
    "                               n_estimators = def_n_estimators)\n",
    "    \n",
    "    rf.fit(train[train_col], train[\"cnt\"])\n",
    "    prediction_rf = rf.predict(test[train_col])\n",
    "    \n",
    "    mse = mean_squared_error(test[\"cnt\"], prediction_rf)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1618.5442035457422,\n",
       " 2: 1640.561528205265,\n",
       " 3: 1701.6152002980284,\n",
       " 4: 1766.477215599489,\n",
       " 5: 1828.675766380393,\n",
       " 6: 1887.3095481138616,\n",
       " 7: 1947.0436142836168,\n",
       " 8: 2007.368500681667,\n",
       " 9: 2059.5154396805997,\n",
       " 10: 2108.2754406197064,\n",
       " 11: 2154.739365463783,\n",
       " 12: 2206.9101456167127,\n",
       " 13: 2254.8178587471034,\n",
       " 14: 2299.6578071122035}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses = {}\n",
    "for i in range(1,15):\n",
    "    mses[i] = forest_train_test(train_cols, train, test, def_min_samples_leaf=i)\n",
    "mses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the random forest performs best with the min_samples_leaf parameter set at 1.\n",
    "#Then the model performs the most accurate result of 1618 mse, lower than any other error produced\n",
    "#by the models tested above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
